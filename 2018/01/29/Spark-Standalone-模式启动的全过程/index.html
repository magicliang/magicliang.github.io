<!DOCTYPE html><html lang="zh-Hans" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Spark Standalone 模式启动的全过程 | 守株阁</title><meta name="author" content="magicliang"><meta name="copyright" content="magicliang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="把这个事情做成一个小 routine，免得以后每次都要看英文文档来搭 dev 环境 准备工作 下载安装包，解压并进入根目录。 .&#x2F;sbin&#x2F;start-master.sh。看 jps 果然已经有了一个 Master 进，文档里面说会打印出 spark 的 master url，但没打印出来。就去默认的http:&#x2F;&#x2F;localhost:8080上看即可： 12URL: spark:&#x2F;&#x2F;magicli">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark Standalone 模式启动的全过程">
<meta property="og:url" content="https://magicliang.github.io/2018/01/29/Spark-Standalone-%E6%A8%A1%E5%BC%8F%E5%90%AF%E5%8A%A8%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/index.html">
<meta property="og:site_name" content="守株阁">
<meta property="og:description" content="把这个事情做成一个小 routine，免得以后每次都要看英文文档来搭 dev 环境 准备工作 下载安装包，解压并进入根目录。 .&#x2F;sbin&#x2F;start-master.sh。看 jps 果然已经有了一个 Master 进，文档里面说会打印出 spark 的 master url，但没打印出来。就去默认的http:&#x2F;&#x2F;localhost:8080上看即可： 12URL: spark:&#x2F;&#x2F;magicli">
<meta property="og:locale">
<meta property="og:image" content="https://magicliang.github.io/img/wall-paper-26.jpg">
<meta property="article:published_time" content="2018-01-29T08:20:15.000Z">
<meta property="article:modified_time" content="2026-01-24T07:32:06.474Z">
<meta property="article:author" content="magicliang">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://magicliang.github.io/img/wall-paper-26.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Spark Standalone 模式启动的全过程",
  "url": "https://magicliang.github.io/2018/01/29/Spark-Standalone-%E6%A8%A1%E5%BC%8F%E5%90%AF%E5%8A%A8%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/",
  "image": "https://magicliang.github.io/img/wall-paper-26.jpg",
  "datePublished": "2018-01-29T08:20:15.000Z",
  "dateModified": "2026-01-24T07:32:06.474Z",
  "author": [
    {
      "@type": "Person",
      "name": "magicliang",
      "url": "https://magicliang.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://magicliang.github.io/2018/01/29/Spark-Standalone-%E6%A8%A1%E5%BC%8F%E5%90%AF%E5%8A%A8%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'undefined')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'undefined')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":50,"languages":{"author":"Author: magicliang","link":"Link: ","source":"Source: 守株阁","info":"Copyright belongs to the author. For commercial use, please contact the author for authorization. For non-commercial use, please indicate the source."}},
  lightbox: 'null',
  Snackbar: {"chs_to_cht":"You have switched to Traditional Chinese","cht_to_chs":"You have switched to Simplified Chinese","day_to_night":"You have switched to Dark Mode","night_to_day":"You have switched to Light Mode","bgLight":"#49b1f5","bgDark":"#2d3035","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: true,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spark Standalone 模式启动的全过程',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 8.1.1"><link rel="alternate" href="/atom.xml" title="守株阁" type="application/atom+xml">
<link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      if ($loadingBox.classList.contains('loaded')) return
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()

  if (document.readyState === 'complete') {
    preloader.endLoading()
  } else {
    window.addEventListener('load', preloader.endLoading)
    document.addEventListener('DOMContentLoaded', preloader.endLoading)
    // Add timeout protection: force end after 7 seconds
    setTimeout(preloader.endLoading, 7000)
  }

  if (false) {
    btf.addGlobalFn('pjaxSend', preloader.initLoading, 'preloader_init')
    btf.addGlobalFn('pjaxComplete', preloader.endLoading, 'preloader_end')
  }
})()</script><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/wall-paper-26.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">守株阁</span></a><a class="nav-page-title" href="/"><span class="site-name">Spark Standalone 模式启动的全过程</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">Spark Standalone 模式启动的全过程</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2018-01-29T08:20:15.000Z" title="Created 2018-01-29 16:20:15">2018-01-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-01-24T07:32:06.474Z" title="Updated 2026-01-24 15:32:06">2026-01-24</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word Count:</span><span class="word-count">1.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading Time:</span><span>7mins</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>把这个事情做成一个小 routine，免得以后每次都要看英文文档来搭 dev 环境</p>
<h2 id="准备工作">准备工作</h2>
<p>下载安装包，解压并进入根目录。</p>
<p><code>./sbin/start-master.sh</code>。看 jps 果然已经有了一个 Master 进，文档里面说会打印出 spark 的 master url，但没打印出来。就去默认的<code>http://localhost:8080</code>上看即可：</p>
<figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">URL:</span> spark:<span class="hljs-comment">//magicliang:7077</span><br>REST URL: spark:<span class="hljs-comment">//magicliang:6066 (cluster mode)</span><br></code></pre></td></tr></table></figure>
<p>这个6066在本地 telnet 不通，也是很神奇。</p>
<p>把这个 URL 拼接成 worker  的启动命令<code>./start-slave.sh spark://magicliang:7077</code>，然后可以看到以下这张图：</p>
<p><img src="https://ws1.sinaimg.cn/large/66dd581fly1fnwjxcayiwj22720wo0zj.jpg" alt=""></p>
<p>文档里的给出的定义 worker 节点的方法：在 Spark 根目录下定义一个 conf/slaves 的文件，每一行写一个主机名。如果这个文件不存在（就是我们现在这个状况），则 worker 就会全部启动在 localhost 上。而 master 是通过 ssh 跟 worker 通信的。默认情况下，ssh 是并行执行，而且要求免密码登录。如果不能提供免密码，要配置一个环境变量 SPARK_SSH_FOREGROUND 并显式地为每个 worker 提供密码。</p>
<p>sbin 里自带了一大套脚本：</p>
<blockquote>
<p>sbin/start-master.sh - Starts a master instance on the machine the script is executed on.<br>
sbin/start-slaves.sh - Starts a slave instance on each machine specified in the conf/slaves file.<br>
sbin/start-slave.sh - Starts a slave instance on the machine the script is executed on.<br>
sbin/start-all.sh - Starts both a master and a number of slaves as described above.<br>
sbin/stop-master.sh - Stops the master that was started via the sbin/start-master.sh script.<br>
sbin/stop-slaves.sh - Stops all slave instances on the machines specified in the conf/slaves file.<br>
sbin/stop-all.sh - Stops both the master and the slaves as described above.</p>
</blockquote>
<p>master 和 worker 相关脚本都支持以下参数：</p>
<blockquote>
<p>-h HOST, --host HOST  Hostname to listen on<br>
-i HOST, --ip HOST  Hostname to listen on (deprecated, use -h or --host)<br>
-p PORT, --port PORT    Port for service to listen on (default: 7077 for master, random for worker)<br>
–webui-port PORT   Port for web UI (default: 8080 for master, 8081 for worker)<br>
-c CORES, --cores CORES Total CPU cores to allow Spark applications to use on the machine (default: all available); only on worker<br>
-m MEM, --memory MEM    Total amount of memory to allow Spark applications to use on the machine, in a format like 1000M or 2G (default: your machine’s total RAM minus 1 GB); only on worker<br>
-d DIR, --work-dir DIR  Directory to use for scratch space and job output logs (default: SPARK_HOME/work); only on worker<br>
–properties-file FILE  Path to a custom Spark properties file to load (default: conf/spark-defaults.conf)</p>
</blockquote>
<p>我们还可以通过在<code>conf/spark-env</code>里设置环境变量进一步配置集群。可以通过<code>conf/spark-env.sh.template</code>来设置初始的 worker 配置，然后把改过的配置拷贝到 worker 机器上去（<strong>TODO: 换言之 Spark 也像 Hadoop 一样要求 Master 和 Worker 的目录结构同构？改天试试。</strong>）。</p>
<h2 id="把应用程序连接到集群上">把应用程序连接到集群上</h2>
<p>两个选择：</p>
<ul>
<li>把<code>spark://magicliang:7077</code>传递给 SparkContext constructor。</li>
<li>直接开 Spark Shell 来连集群：<code>./bin/spark-shell --master spark://magicliang:7077</code>。这时候就会启动一个被修改过的 scala repl 环境</li>
</ul>
<p>在 shell 环境里输入：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-comment">// 可以看到，在 Spark 中（还没有进入 Spark SQL 模块）默认的数据对象就是 Dataset</span><br> <span class="hljs-keyword">val</span> textFile = spark.read.textFile(<span class="hljs-string">&quot;README.md&quot;</span>)<br><span class="hljs-comment">//18/01/28 20:21:17 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException</span><br><span class="hljs-comment">//textFile: org.apache.spark.sql.Dataset[String] = [value: string]</span><br><br>textFile.count() <span class="hljs-comment">// Number of items in this Dataset</span><br><span class="hljs-comment">//res0: Long = 126 // May be different from yours as README.md will change over time, similar to other outputs</span><br><br>textFile.first() <span class="hljs-comment">// First item in this Dataset</span><br><span class="hljs-comment">// 其实这里面第一个元素，就是第一行的意思</span><br><span class="hljs-comment">//res1: String = # Apache Spark</span><br><br><span class="hljs-comment">// 从侧面也可以看出，这里面数据集的单个元素是一行</span><br><span class="hljs-keyword">val</span> linesWithSpark = textFile.filter(line =&gt; line.contains(<span class="hljs-string">&quot;Spark&quot;</span>))<br><span class="hljs-comment">//linesWithSpark: org.apache.spark.sql.Dataset[String] = [value: string]</span><br>linesWithSpark.count()<br><span class="hljs-comment">//res1: Long = 20</span><br><br><span class="hljs-comment">// 寻找有最多词语的行</span><br><span class="hljs-comment">// Dataset 和 Dataset 之间可以直接做变换，不需要用到什么 converter。</span><br><span class="hljs-comment">// 这也是个用 reduce 化多为一的操作</span><br>textFile.map(line =&gt; line.split(<span class="hljs-string">&quot; &quot;</span>).size).reduce((a, b) =&gt; <span class="hljs-keyword">if</span> (a &gt; b) a <span class="hljs-keyword">else</span> b)<br><span class="hljs-comment">// res2: Int = 22</span><br><span class="hljs-comment">// 上一行代码也可以这样写：</span><br>textFile.map(line =&gt; line.split(<span class="hljs-string">&quot; &quot;</span>).size).reduce((a, b) =&gt; <span class="hljs-type">Math</span>.max(a, b))<br><br><span class="hljs-comment">// 经典的 wordcount 问题的一行解</span><br><span class="hljs-keyword">val</span> wordCounts = textFile.flatMap(line =&gt; line.split(<span class="hljs-string">&quot; &quot;</span>)).groupByKey(identity).count()<br><span class="hljs-comment">// wordCounts: org.apache.spark.sql.Dataset[(String, Long)] = [value: string, count(1): bigint]</span><br><br><span class="hljs-comment">// 经典的收束解</span><br>wordCounts.collect()<br>res6: <span class="hljs-type">Array</span>[(<span class="hljs-type">String</span>, <span class="hljs-type">Long</span>)] = <span class="hljs-type">Array</span>((online,<span class="hljs-number">1</span>), (graphs,<span class="hljs-number">1</span>), ([<span class="hljs-string">&quot;Parallel,1), ([&quot;</span><span class="hljs-type">Building</span>,<span class="hljs-number">1</span>), (thread,<span class="hljs-number">1</span>), (documentation,<span class="hljs-number">3</span>), (command,,<span class="hljs-number">2</span>), (abbreviated,<span class="hljs-number">1</span>), (overview,<span class="hljs-number">1</span>), (rich,<span class="hljs-number">1</span>), (set,<span class="hljs-number">2</span>), (-<span class="hljs-type">DskipTests</span>,<span class="hljs-number">1</span>), (name,<span class="hljs-number">1</span>), (page](http:<span class="hljs-comment">//spark.apache.org/documentation.html).,1), ([&quot;Specifying,1), (stream,1), (run:,1), (not,1), (programs,2), (tests,2), (./dev/run-tests,1), (will,1), ([run,1), (particular,2), (option,1), (Alternatively,,1), (by,1), (must,1), (using,5), (you,4), (MLlib,1), (DataFrames,,1), (variable,1), (Note,1), (core,1), (more,1), (protocols,1), (guidance,2), (shell:,2), (can,7), (site,,1), (systems.,1), (Maven,1), ([building,1), (configure,1), (for,12), (README,1), (Interactive,2), (how,3), ([Configuration,1), (Hive,2), (system,1), (provides,1), (Hadoop-supported,1), (pre-built,1...</span><br><br><span class="hljs-comment">// 显式地指定一个数据集为全集群内的 in memory 缓存，也就是说，这个缓存不是默认生效的</span><br>linesWithSpark.cache()<br><span class="hljs-comment">//res7: linesWithSpark.type = [value: string]</span><br></code></pre></td></tr></table></figure>
<p>最后一个缓存是特别有意思的地方。即使这些数据是分布在整个集群的各个地方的，Spark 也有办法把它缓存起来。</p>
<h2 id="开始写一个小小的程序">开始写一个小小的程序</h2>
<p>建一个新项目<code>sbt new sbt/scala-seed.g8</code>，输入项目名 first-app。</p>
<p>或者用 idea 建立一个项目。然后把相关代码写进去。</p>
<p>重点关注几个文件。</p>
<p>Dependencies.scala(这个文件就是下面文件里面 import 的对象)</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">import</span> sbt._<br><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">Dependencies</span> </span>&#123;<br>  <span class="hljs-keyword">lazy</span> <span class="hljs-keyword">val</span> scalaTest = <span class="hljs-string">&quot;org.scalatest&quot;</span> %% <span class="hljs-string">&quot;scalatest&quot;</span> % <span class="hljs-string">&quot;3.0.3&quot;</span><br>&#125;<br><br></code></pre></td></tr></table></figure>
<p>build.sbt</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">import</span> <span class="hljs-type">Dependencies</span>._<br><br><span class="hljs-keyword">lazy</span> <span class="hljs-keyword">val</span> root = (project in file(<span class="hljs-string">&quot;.&quot;</span>)).<br>  settings(<br>    inThisBuild(<span class="hljs-type">List</span>(<br>      organization := <span class="hljs-string">&quot;com.magicliang&quot;</span>,<br>      <span class="hljs-comment">// 高版本的 scala 仓库里下载不到这个 spark-sql 模块</span><br>      scalaVersion := <span class="hljs-string">&quot;2.11.8&quot;</span>,<br>      version      := <span class="hljs-string">&quot;0.1&quot;</span><br>    )),<br>    name := <span class="hljs-string">&quot;first-application&quot;</span>,<br>    libraryDependencies += scalaTest % <span class="hljs-type">Test</span>,<br>    libraryDependencies += <span class="hljs-string">&quot;org.apache.spark&quot;</span> %% <span class="hljs-string">&quot;spark-sql&quot;</span> % <span class="hljs-string">&quot;2.2.1&quot;</span><br>  )<br></code></pre></td></tr></table></figure>
<p>具体的项目代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.magicliang<br><br><span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">SparkSession</span><br><br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment">  * @author liangchuan</span><br><span class="hljs-comment">  */</span><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">SimpleApplication</span> </span>&#123;<br><br>  println(<span class="hljs-string">&quot;hello world&quot;</span>)<br><br>  <span class="hljs-comment">// http://blog.csdn.net/csdn_chuxuezhe/article/details/75351762</span><br>  <span class="hljs-comment">// 在 VM options 里增加 -Dspark.master=local 即可指定应用程序在本地单线程启动。</span><br>  <span class="hljs-comment">// 暂时不知道为什么用 ide 启动只支持 local[k] 模式而不支持提交任务到 master url 的模式</span><br>  <span class="hljs-comment">// https://spark.apache.org/docs/latest/submitting-applications.html spark 的几种启动模式参数，也可以在 submit 里指定</span><br>  <span class="hljs-comment">// Spark 模式的总结：https://www.jianshu.com/p/65a3476757a5</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>    <span class="hljs-keyword">val</span> logFile = <span class="hljs-string">&quot;/Users/magicliang/Desktop/Programming/tools/spark-2.2.1-bin-hadoop2.7/README.md&quot;</span><br>    <span class="hljs-keyword">val</span> spark = <span class="hljs-type">SparkSession</span>.builder.appName(<span class="hljs-string">&quot;SimpleApplication&quot;</span>).getOrCreate()<br>    println(<span class="hljs-string">&quot;SparkSession created&quot;</span>)<br>    <span class="hljs-keyword">val</span> logData = spark.read.textFile(logFile).cache()<br>    println(<span class="hljs-string">&quot;Dataset created and cached&quot;</span>)<br>    <span class="hljs-keyword">val</span> numAs = logData.filter(line =&gt; line.contains(<span class="hljs-string">&quot;a&quot;</span>)).count()<br>    <span class="hljs-keyword">val</span> numBs = logData.filter(line =&gt; line.contains(<span class="hljs-string">&quot;b&quot;</span>)).count()<br>    <span class="hljs-comment">// 这里这个 s 开头字符串本身就是可替换字符串的前缀，没有它是不可以做到字符串替换的。</span><br>    println(<span class="hljs-string">s&quot;Lines with a: <span class="hljs-subst">$numAs</span>, Lines with b: <span class="hljs-subst">$numBs</span>&quot;</span>)<br>    <span class="hljs-type">Thread</span>.sleep(<span class="hljs-number">20000</span>)<br>    spark.stop()<br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>
<p>完整的目标工程见<a target="_blank" rel="noopener" href="https://github.com/magicliang/SparkStudy/tree/master/first-application">此</a>。</p>
<p>sbt 部分的<a target="_blank" rel="noopener" href="https://www.scala-sbt.org/1.x/docs/Hello.html">参考链接</a>。<br>
本文主要参考 Spark 的<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/quick-start.html">quickstart</a>。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://magicliang.github.io">magicliang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://magicliang.github.io/2018/01/29/Spark-Standalone-%E6%A8%A1%E5%BC%8F%E5%90%AF%E5%8A%A8%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/">https://magicliang.github.io/2018/01/29/Spark-Standalone-%E6%A8%A1%E5%BC%8F%E5%90%AF%E5%8A%A8%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><div class="post-share"><div class="social-share" data-image="/img/wall-paper-26.jpg" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2018/01/28/DAG-%E6%89%A7%E8%A1%8C%E6%A1%86%E6%9E%B6%E4%BC%98%E4%BA%8E-MapReduce-%E7%9A%84%E5%9C%B0%E6%96%B9%E5%9C%A8%E5%93%AA%E9%87%8C%EF%BC%9F/" title="DAG 执行框架优于 MapReduce 的地方在哪里？"><img class="cover" src="/img/wall-paper-176.jpeg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2018-01-28</div><div class="info-item-2">DAG 执行框架优于 MapReduce 的地方在哪里？</div></div><div class="info-2"><div class="info-item-1">有个同学问我什么是 DAG 框架。我感觉隐隐约约听过，但又讲不清楚它的概念。 上网搜了一下，我们常见的新大数据执行框架如 Spark、Storm，还有一个我没听过的 Tez，都算 DAG 任务执行框架。他们的主要优点是，可以用 DAG 事先通晓整个任务的全部步骤，然后进行转换优化。如 Tez 就可以把多个任务转换为一个大任务，而 Spark 则可以把相关联的 Map 直接串联起来， 免得多次写回 hdfs（看来 hdfs 也很慢）。传统的 MapReduce 框架为什么不能理解这种优化空间的存在，在任务运行的时候好像一个盲人一样，是个很有意思的话题。 Quora 上的一个相关的问答。 </div></div></div></a><a class="pagination-related" href="/2020/04/16/Spark-SQL-%E5%8E%9F%E7%90%86/" title="Spark SQL 原理"><img class="cover" src="/img/wall-paper-75.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2020-04-16</div><div class="info-item-2">Spark SQL 原理</div></div><div class="info-2"><div class="info-item-1">Spark SQL的发展历程 为了给熟悉的 RDBMS 但又不理解 MapReduce 的技术人员提供快速上手的工具，Hive 应运而生，他是当时唯一运行在 Hadoop 上的SQL-On-Hadoop 工具。 但是 MapReduce 计算过程中大量的中间磁盘落地过程消耗了大量的 I/O，降低的运行效率，为了提高 SQL 的执行效率，大量的 SQL-On-Hadoop工具开始产生，而 Shark 是其中一个表现较为突出的项目。 Shark是伯克利实验室 Spark 生态环境的组件之一，它主要修改了内存管理，物理计划和执行三个模块，值得它能运行在 Spark 的引擎上，从而提高 SQL 查询的效率。 但是随着 Spark 的发展，Shark 对 Hive 过多的依赖制约了 Spark 的设计理念和各个组件之间的相互继承，所以 Spark 团队停止了对 Shark 的开发，提出了 SparkSQL 项目。 因为摆脱了Hive 的过度依赖，Spark SQL在数据兼容性，性能优化和组件扩展等各个方面都得到了极大的方便和发展。 提出了 SparkSQL 项目之后，SQL On Spar...</div></div></div></a><a class="pagination-related" href="/2025/07/29/%E7%BB%8F%E5%85%B8%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E7%9A%84%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%B3%95/" title="经典面试问题的大数据解法——Spark 与 Flink 实战"><img class="cover" src="/img/wall-paper-9.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-29</div><div class="info-item-2">经典面试问题的大数据解法——Spark 与 Flink 实战</div></div><div class="info-2"><div class="info-item-1">“100 亿个数中找出最大的 1000 个”、“两个 10GB 的文件找出共同的 URL”——这些经典面试题的本质都是内存放不下。本文将系统性地梳理大数据场景下的 TopK、排序、去重、词频统计等经典问题，给出从单机到分布式（Spark/Flink）的完整解法。  引言：大数据问题的共同特征 为什么&quot;内存放不下&quot;    数据规模 内存需求 典型服务器内存 能否放入内存     1 亿个 int 400 MB 16 GB ✅   10 亿个 int 4 GB 16 GB ✅   100 亿个 int 40 GB 16 GB ❌   10 亿个 URL（平均 100 字节） 100 GB 16 GB ❌    通用解题框架 123456789101112大数据问题    │    ▼能否放入内存？    ├── 能 → 直接用内存数据结构解决    │    ▼    不能 → 分而治之    │    ├── 1. 分区（Partition）：按哈希/范围将数据分成小块    ├── 2. 局部处理：每个小块在内存中处理    └── 3. 合并（Merge）：...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C"><span class="toc-number">1.</span> <span class="toc-text">准备工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8A%8A%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E8%BF%9E%E6%8E%A5%E5%88%B0%E9%9B%86%E7%BE%A4%E4%B8%8A"><span class="toc-number">2.</span> <span class="toc-text">把应用程序连接到集群上</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%80%E5%A7%8B%E5%86%99%E4%B8%80%E4%B8%AA%E5%B0%8F%E5%B0%8F%E7%9A%84%E7%A8%8B%E5%BA%8F"><span class="toc-number">3.</span> <span class="toc-text">开始写一个小小的程序</span></a></li></ol></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2017 - 2026 By magicliang</span><span class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.1.1</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.2</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Toggle Between Traditional and Simplified Chinese">簡</button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.2"></script><script src="/js/main.js?v=5.5.2"></script><script src="/js/tw_cn.js?v=5.5.2"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const config = mermaidSrc.dataset.config ? JSON.parse(mermaidSrc.dataset.config) : {}
      if (!config.theme) {
        config.theme = theme
      }
      const mermaidThemeConfig = `%%{init: ${JSON.stringify(config)}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.12.1/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="/"></script></div></body></html>