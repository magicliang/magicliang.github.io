<!DOCTYPE html><html lang="zh-Hans" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Spark SQL 原理 | 守株阁</title><meta name="author" content="magicliang"><meta name="copyright" content="magicliang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Spark SQL的发展历程 为了给熟悉的 RDBMS 但又不理解 MapReduce 的技术人员提供快速上手的工具，Hive 应运而生，他是当时唯一运行在 Hadoop 上的SQL-On-Hadoop 工具。 但是 MapReduce 计算过程中大量的中间磁盘落地过程消耗了大量的 I&#x2F;O，降低的运行效率，为了提高 SQL 的执行效率，大量的 SQL-On-Hadoop工具开始产生，而 Shark">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark SQL 原理">
<meta property="og:url" content="https://magicliang.github.io/2020/04/16/Spark-SQL-%E5%8E%9F%E7%90%86/index.html">
<meta property="og:site_name" content="守株阁">
<meta property="og:description" content="Spark SQL的发展历程 为了给熟悉的 RDBMS 但又不理解 MapReduce 的技术人员提供快速上手的工具，Hive 应运而生，他是当时唯一运行在 Hadoop 上的SQL-On-Hadoop 工具。 但是 MapReduce 计算过程中大量的中间磁盘落地过程消耗了大量的 I&#x2F;O，降低的运行效率，为了提高 SQL 的执行效率，大量的 SQL-On-Hadoop工具开始产生，而 Shark">
<meta property="og:locale">
<meta property="og:image" content="https://magicliang.github.io/img/wall-paper-4.jpg">
<meta property="article:published_time" content="2020-04-16T10:06:24.000Z">
<meta property="article:modified_time" content="2026-01-24T07:32:06.932Z">
<meta property="article:author" content="magicliang">
<meta property="article:tag" content="Java">
<meta property="article:tag" content="Scala">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://magicliang.github.io/img/wall-paper-4.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Spark SQL 原理",
  "url": "https://magicliang.github.io/2020/04/16/Spark-SQL-%E5%8E%9F%E7%90%86/",
  "image": "https://magicliang.github.io/img/wall-paper-4.jpg",
  "datePublished": "2020-04-16T10:06:24.000Z",
  "dateModified": "2026-01-24T07:32:06.932Z",
  "author": [
    {
      "@type": "Person",
      "name": "magicliang",
      "url": "https://magicliang.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://magicliang.github.io/2020/04/16/Spark-SQL-%E5%8E%9F%E7%90%86/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'undefined')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'undefined')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":50,"languages":{"author":"Author: magicliang","link":"Link: ","source":"Source: 守株阁","info":"Copyright belongs to the author. For commercial use, please contact the author for authorization. For non-commercial use, please indicate the source."}},
  lightbox: 'null',
  Snackbar: {"chs_to_cht":"You have switched to Traditional Chinese","cht_to_chs":"You have switched to Simplified Chinese","day_to_night":"You have switched to Dark Mode","night_to_day":"You have switched to Light Mode","bgLight":"#49b1f5","bgDark":"#2d3035","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: true,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spark SQL 原理',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 8.1.1"><link rel="alternate" href="/atom.xml" title="守株阁" type="application/atom+xml">
<link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      if ($loadingBox.classList.contains('loaded')) return
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()

  if (document.readyState === 'complete') {
    preloader.endLoading()
  } else {
    window.addEventListener('load', preloader.endLoading)
    document.addEventListener('DOMContentLoaded', preloader.endLoading)
    // Add timeout protection: force end after 7 seconds
    setTimeout(preloader.endLoading, 7000)
  }

  if (false) {
    btf.addGlobalFn('pjaxSend', preloader.initLoading, 'preloader_init')
    btf.addGlobalFn('pjaxComplete', preloader.endLoading, 'preloader_end')
  }
})()</script><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/wall-paper-4.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">守株阁</span></a><a class="nav-page-title" href="/"><span class="site-name">Spark SQL 原理</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">Spark SQL 原理</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2020-04-16T10:06:24.000Z" title="Created 2020-04-16 18:06:24">2020-04-16</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-01-24T07:32:06.932Z" title="Updated 2026-01-24 15:32:06">2026-01-24</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word Count:</span><span class="word-count">2.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading Time:</span><span>8mins</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1>Spark SQL的发展历程</h1>
<p>为了给熟悉的 RDBMS 但又不理解 MapReduce 的技术人员提供快速上手的工具，Hive 应运而生，他是当时唯一运行在 Hadoop 上的SQL-On-Hadoop 工具。 但是 MapReduce 计算过程中大量的中间磁盘落地过程消耗了大量的 I/O，降低的运行效率，为了提高 SQL 的执行效率，大量的 SQL-On-Hadoop工具开始产生，而 Shark 是其中一个表现较为突出的项目。</p>
<p>Shark是伯克利实验室 Spark 生态环境的组件之一，它主要修改了内存管理，物理计划和执行三个模块，值得它能运行在 Spark 的引擎上，从而提高 SQL 查询的效率。</p>
<p>但是随着 Spark 的发展，Shark 对 Hive 过多的依赖制约了 Spark 的设计理念和各个组件之间的相互继承，所以 Spark 团队停止了对 Shark 的开发，提出了 SparkSQL 项目。 因为摆脱了Hive 的过度依赖，Spark SQL在数据兼容性，性能优化和组件扩展等各个方面都得到了极大的方便和发展。</p>
<p>提出了 SparkSQL 项目之后，SQL On Spark 发展出了两条支线，SparkSQL 和Hive on Spark。</p>
<h1>Spark SQL 解析流程</h1>
<p>Spark SQL 对 SQL 语句的处理和关系型数据库类似，即词法/语法解析，绑定，优化，执行。 Spark SQL 会先将 SQL 语句解析成一棵树，然后使用各种规则对 Tree 进行绑定和优化等处理。</p>
<p><img src="spark-sql-parsing%E3%80%82jpeg" alt="spark-sql-parsing。jpeg"></p>
<h2 id="使用sessioncatalog保存元数据">使用SessionCatalog保存元数据</h2>
<p>在解析 SQL 语句之前，会创建 SparkSession，在 Spark2。0版本之后，SparkSession封装了 SparkContext 和 SQLContext 创建，也不再区分 SQLContext 和 HiveContext。</p>
<p>涉及到诸如表名，字段名称和字段类型等元数据都会保存在 SessionCatalog 中。 此外，创建临时表或者试图的过程，其实就会往 SessionCatalog 注册。</p>
<h2 id="解析-sql-使用-antlr-生成未解析的逻辑计划">解析 SQL: 使用 ANTLR 生成未解析的逻辑计划</h2>
<p>只要是在数据库类型的技术里面，比如传统的 MySQL，Oracle 或者现在大数据领域的Hive。 它的基本的 SQL 执行的模型，都是类似的，首先都是要生成一条 SQL 语句的执行计划。</p>
<p>当调用 SparkSession 的 sql 或者 SQLContext 的 sql 方法时，就会使用 SparkSqlParser 进行解析 SQL。 使用的 ANTLR 进行词法解析和语法解析。 它分为词法分析和构建分析树或者语法树 AST 2个步骤来生成UnresolvedLogicalPlan。</p>
<h2 id="使用分析器-analyzer-解析逻辑计划">使用分析器 Analyzer 解析逻辑计划</h2>
<p>在该阶段，Analyzer 会使用 Analyzer Rules，并结合 SessionCatalog，对未解析的逻辑计划进行解析，生成已解析的逻辑计划。</p>
<h2 id="使用优化器-optimizer-优化逻辑计划">使用优化器 Optimizer 优化逻辑计划</h2>
<p>优化器也是会定义一套 Rules，利用这些 Rules 对逻辑计划和 Expression 进行迭代处理，从而使得树的节点进行优化。 在传统的 Oracle 等数据库中，通常都会生成多个执行计划，然后根据优化器针对多个计划选择一个最好的计划。 而SparkSQL中，是对一个已生成的执行进行优化生成一个新的执行计划。</p>
<p>比如，一个 SQL 语句</p>
<figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs lasso"><span class="hljs-keyword">select</span> name from (<br><br>    <span class="hljs-keyword">select</span> <span class="hljs-params">...</span> from <span class="hljs-params">...</span>)<br><br><span class="hljs-keyword">where</span> <span class="hljs-params">...</span> = <span class="hljs-params">...</span><br></code></pre></td></tr></table></figure>
<p>此时，在执行计划解析出来的时候。 其实就是按照sql 原封不动的样子来解析成可以执行的计划的。 但是Optimizer会对执行计划进行优化。 在这个例子中的 where 条件是可以放入子查询中的，这样子查询的数据量大大变小，可以优化执行速度。 相应的执行计划就会变为</p>
<figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs lasso"><span class="hljs-keyword">select</span> name from (<br><br>    <span class="hljs-keyword">select</span> name from <span class="hljs-params">...</span>. <span class="hljs-keyword">where</span> <span class="hljs-params">...</span> = <span class="hljs-params">...</span>)<br></code></pre></td></tr></table></figure>
<h2 id="使用sparkplanner生成物理计划">使用SparkPlanner生成物理计划</h2>
<p>SparkPlanner使用 Planning Strategies，对优化后的逻辑计划进行转换，生成可以执行的物理计划SparkPlan。</p>
<p>生成的物理计划区别于逻辑计划的是执行计划在这里已经非常清晰了。 从哪个文件读取什么数据，从哪里读取数据，如何操作等等都已经明确了。</p>
<h2 id="使用-queryexecution-执行物理计划">使用 QueryExecution 执行物理计划</h2>
<p>此时调用 SparkPlan 的 execute 方法，底层其实已经在触发创建并执行 Job 了，然后返回相应结果。</p>
<h1>三 SparkSQL中物理计划的执行</h1>
<h2 id="spark-的根基-rdd">Spark 的根基: RDD</h2>
<p>与许多专有的大数据处理平台不同，Spark 建立在统一抽象的 RDD 之上，使得它可以以基本一致的方式应对不同的大数据处理场景。</p>
<p>也就是说，要理解SparkSQL，首先要理解 SparkRDD。</p>
<p><img src="sparkRDD%E3%80%82png" alt="sparkRDD。png"></p>
<p>RDD的全称是 Resilient Distributed Datasets，是一个容错的，并行的数据结构，可以让用户显示的将数据存储到磁盘和内存中。 同时，RDD 还提供了一组丰富的操作来操作这些数据。 在这些操作中，诸如 map，flatMap，filter 等转换操作实现了monad模式，很好地契合了Scala的集合操作。除此之外，RDD还提供了诸如join、groupBy、reduceByKey等更为方便的操作（注意，reduceByKey是action，而非transformation），以支持常见的数据运算。</p>
<p>RDD提供了两方面的特性persistence和patitioning，用户可以通过persist与patitionBy函数来控制RDD的这两个方面。RDD的分区特性与并行计算能力(RDD定义了parallerize函数)，使得Spark可以更好地利用可伸缩的硬件资源。若将分区与持久化二者结合起来，就能更加高效地处理海量数据。</p>
<p>既然 RDD 是 spark job 的根基，所有 Spark 的计算都是基于 RDD 的相应Transformation 和 Action 操作。 Spark的整个生态系统与Hadoop是完全兼容的,所以对于Hadoop所支持的文件类型或者数据库类型,Spark也同样支持。  Spark提供了可以读取 Hadoop 文件的接口，对于外部存储创建而言，hadoopRDD 函数是最常用的在 Spark 中创建 RDD 的接口。 也就是说用户可以用这个接口读取集群中的数据并转换成相应的 RDD，然后接入 spark 计算引擎中。</p>
<h2 id="从物理执行计划到-rdd">从物理执行计划到 RDD</h2>
<p>经过 Spark 的 Catalyst 解析优化器对于 sql 的分析之后，已经生成了相应的物理执行计划，而对于已经可以开始执行的 SparkSQL的 Physical Plan，其执行目标就是调用 action 算子之后的 RDD。</p>
<p>SparkPlan主要包含四种操作类型:</p>
<ul>
<li>BasicOperator</li>
<li>Join</li>
<li>Aggregate</li>
<li>Sort</li>
</ul>
<p>对于 SparkSQL 的物理执行计划，基本可以通过这四种操作类型来完成。 Spark 会以这四种操作为基础创建出相应的 task来分别执行。 这四种操作类型的详细介绍这里不做过多阐述。</p>
<h1>SparkSQL中任务的执行模式</h1>
<p>在介绍了 SparkSQL 如何生成相应的物理执行计划以及物理执行计划是如何执行之后，SparkSQL 任务就进入了具体的执行阶段。 SparkSQL 具体任务的执行方式和普通 Spark 任务的执行方式并无差别。 Spark 任务运行时的角色可以分为三种: driver，executor(worker) 以及cluster manager。</p>
<p><img src="cluster%E3%80%82png" alt="cluster。png"></p>
<p>Spark中的driver其实和yarn中Application Master的功能相类似。主要完成任务的调度以及和executor和cluster manager进行协调。</p>
<p>在YARN中，每个Application实例都有一个Application Master进程，它是Application启动的第一个容器。它负责和ResourceManager打交道，并请求资源。获取资源之后告诉NodeManager为其启动container。如果对 MapReduce 比较熟悉的话，一定不会对 Applicaiton Master 陌生。 在 Spark on Yarn 中，任务的执行模式可以分为 yarn-client 和 yarn-cluster 两种。</p>
<p>从深层次的含义讲，yarn-cluster和yarn-client模式的区别其实就是Application Master进程的区别，yarn-cluster模式下，driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行。然而yarn-cluster模式不适合运行交互类型的作业。而yarn-client模式下，Application Master仅仅向YARN请求executor，client会和请求的container通信来调度他们工作，也就是说Client不能离开。看下下面的两幅图应该会明白（上图是yarn-cluster模式，下图是yarn-client模式）：</p>
<p><img src="yarn-cluster%E3%80%82png" alt="yarn-cluster。png"></p>
<h1>SparkSQL 中任务的具体执行</h1>
<p>了解了 Spark 任务的执行模式之后，相应的 job 还不能马上开始运行。 这是因为一个完整的执行计划可能比较复杂，完整的数据量也比较大，直接完整执行不太现实。 通常情况下，一个分布式计算引擎都会将一个完整 job 进行切割分批运行，spark 也是这么做的。</p>
<p>在 Spark 中，task 是一个 Job 进行切割后运行的最小运算单元，一般情况下，一个 rdd 有多少个 partition，就会有多少个 task，因为每一个 task 只是处理一个 partition 上的数据。 当在YARN上运行Spark作业，每个Spark executor作为一个YARN容器(container)也就是 executor运行。Spark可以使得多个Tasks在同一个容器(container)里面运行。这种情况在 spark 中通过核数来控制，这也是一个与 MapReduce 较大不同的地方。</p>
<p>而 task 进行组合分批后，通常称为 stage。 也就是说一个Job会被拆分为多组Task，每组任务被称为一个Stage就像Map Stage， Reduce Stage。Stage的划分在RDD的论文中有详细的介绍，简单的说是以shuffle和result这两种类型来划分。在Spark中有两类task，一类是shuffleMapTask，一类是resultTask，第一类task的输出是shuffle所需数据，第二类task的输出是result，stage的划分也以此为依据，shuffle之前的所有变换是一个stage，shuffle之后的操作是另一个stage。</p>
<p>Spark 会为不同的 stage 以及不同的 task设好前后依赖，来保证整个 job 运行的正确性和完整性。</p>
<p>当最后一个resultTask 的上游 task 全部运行完之后即可开始运行，该resultTask 的结束也意味着 job 的成功运行。</p>
<p>至此，一个 SparkSQL 的任务从解析到生成逻辑计划，生成物理执行计划，再到具体执行返回结果的完整运行原理介绍完毕。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://magicliang.github.io">magicliang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://magicliang.github.io/2020/04/16/Spark-SQL-%E5%8E%9F%E7%90%86/">https://magicliang.github.io/2020/04/16/Spark-SQL-%E5%8E%9F%E7%90%86/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Java/">Java</a><a class="post-meta__tags" href="/tags/Scala/">Scala</a><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><div class="post-share"><div class="social-share" data-image="/img/wall-paper-4.jpg" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2017/10/23/%E6%98%82%E8%B4%B5%E7%9A%84%E5%BC%82%E5%B8%B8/" title="昂贵的异常"><img class="cover" src="/img/wall-paper-31.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2017-10-23</div><div class="info-item-2">昂贵的异常</div></div><div class="info-2"><div class="info-item-1">抛出问题 Joshua Bloch 在《Effective Java》的 Item 57 里明确地提到过，不要试图用 Exception 的跳转来代替正常的程序控制流。他列举了很多原因，但特别提到了抛出异常会使得整个程序运行变慢。抛出异常远比普通的 return、break 等操作对控制流、数据流的性能影响要大，它就只适合拿来作异常分支的控制语句，而不能拿来编写正常的逻辑。  Throwing exception is expensive.  这句话在 Java 的程序员世界里面已经成为老生常谈，却很少有人谈及到底抛出异常比正常的程序跳转返回慢在哪里，有多慢。&quot;不要滥用异常&quot;好像一个猴子定律，人们知道不能这么做，却不明白为什么不能这么做。 此前读了一位同事写的好文《Java虚拟机是如何处理异常的》，深入地分析了 JVM 对异常跳转的处理过程：JVM 会通过异常表的机制，优化异常抛出和正常返回之间的性能差异。仅从程序计数器的移动上来讲，抛出一个异常对栈帧的弹栈并不比直接返回更昂贵。写在前头的结论是：&quot;try-catch 语句块几乎不会影响程序运行性能！...</div></div></div></a><a class="pagination-related" href="/2017/11/30/Java%E4%B8%AD%E7%9A%84%E5%B9%BD%E7%81%B5%E7%B1%BB%E5%9E%8B/" title="Java中的幽灵类型"><img class="cover" src="/img/wall-paper-47.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2017-11-30</div><div class="info-item-2">Java中的幽灵类型</div></div><div class="info-2"><div class="info-item-1">什么是幽灵类型 先上结论：幽灵类型（Phantom Type）顾名思义，就是幽灵般的类型，这种类型往往在运行时可以消失，因为在运行时没有任何作用，它们最大的特点就是没有任何实例（Java 的 Void 就是一个不可实例化类型的例子，常被用作幽灵类型的类型参数，如 Future&lt;Void&gt;）。幽灵类型是一种可以把有些运行时才能检测到的错误，在编译时检测出来的技巧。按照有些老外的观点，就是&quot;Making Wrong Code Look Wrong&quot;。在面向对象的编程语言之中，幽灵类型的实现，往往与状态模式较为接近，但比状态模式提供了更强的纠错功能。在 Java 5 以后的版本里，程序员可以使用泛型。通过泛型的类型参数，Java 中也拥有了幽灵类型的能力。 上面的阐述是不是很难看懂？直接进入具体的例子。假设有一个飞机控制程序，操作飞机起飞或者落地。这个程序有一个非常强的业务约束，就是必须保证飞机一开始必须出现在地上，只有在地上的飞机可以起飞，只有起飞的飞机可以落地，那么应该怎样设计程序（主要是类型关系），来保证这个约束必然成立呢？ 定义状态接口 先来定义...</div></div></div></a><a class="pagination-related" href="/2018/05/29/JPA-%E7%9A%84-id-%E7%94%9F%E6%88%90%E7%AD%96%E7%95%A5/" title="JPA 的 id 生成策略"><img class="cover" src="/img/wall-paper-119.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2018-05-29</div><div class="info-item-2">JPA 的 id 生成策略</div></div><div class="info-2"><div class="info-item-1">JPA 有一个@GeneratedValue注解，有一个strategy attribute，如 @GeneratedValue(strategy = GenerationType.IDENTITY)。 常见的可选策略主要有IDENTITY和SEQUENCE。 GenerationType.IDENTITY 要求底层有一个 integer 或者 bigint 类型的自增列（ auto-incremented column)。自增列的赋值必须在插入操作之后发生，因为这个原因，Hibernate 无法进行各种优化（特别是 JDBC 的 batch 处理，一次 flush 操作会产生很多条insert 语句，分别执行）。如果事务回滚，自增列的值就会被丢弃。数据库在这个自增操作上有个高度优化的轻量级锁机制，性能非常棒。 MySQL 支持这种 id 生成策略， 使用 MySQL 应该尽量使用这个策略，即使它无法优化。 JPA 用它生成 id，会一条一条地插入新的 entity。 GenerationType.SEQUENCE 数据库有一个所谓的 sequence 对象，可以通过 selec...</div></div></div></a><a class="pagination-related" href="/2018/06/19/%E5%A6%82%E4%BD%95%E5%81%9A%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E7%9A%84%E9%97%AE%E9%A2%98%E4%B8%8B%E7%9A%84%E7%AD%94%E6%A1%88/" title="如何做性能测试的问题下的答案"><img class="cover" src="/img/wall-paper-155.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2018-06-19</div><div class="info-item-2">如何做性能测试的问题下的答案</div></div><div class="info-2"><div class="info-item-1">试着回答一下这个问题。 首先要划分系统类型：有状态还是无状态，业务系统还是存储系统。根据不同的业务场景，设立性能测试的目标：是要测 QPS，还是 TPS 还是 TPS，还是任何其他【性能】-从广义来讲，一个存储系统到底能够以多高的平均时延来管理大多的存储空间，可能也是性能的一种。 有了性能测试的目标，接下来就是拆解用例。如果把性能测试归为测试的话，测试就需要测试用例，测试用例只是用例的形式化表达。把用户的使用场景勾勒出来，把每一步拆解成的流程图或者时序图–我们已经得到了一个纸上的集成测试计划，只是没有跟性能挂上钩。 接下来就进入真正写测试用例的环节了。 我们的测试报告如果要涵盖足够立体的信息，则既要了解每一个环节/接口/API 的性能指标，又要了解整体的性能指标。 这个时候测试工具的覆盖面就很重要了。如果我们选择偏黑盒的测试工具，apache ab /JMeter，则我们的测试用例就要围绕着对外交互的 API写，也只能测到外围接口的性能。这样的测试用例写起来最简单，无需侵入任何内部代码中。 如果我们使用了 JMH 一类的工具，则可以自由编写对任何方法的测试用例。但需要对系统有非常...</div></div></div></a><a class="pagination-related" href="/2018/09/07/%E6%97%A5%E6%9C%9F%E4%B8%8E%E6%97%B6%E9%97%B4/" title="日期与时间"><img class="cover" src="/img/wall-paper-8.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2018-09-07</div><div class="info-item-2">日期与时间</div></div><div class="info-2"><div class="info-item-1">JSR 310 Java Date与Time API 新旧 API 的更迭 旧的 Java API 主要包括java.util.Date和java.util.Calendar 两个包的内容。这两个包的时间类型是可变的。如 Date 的实例可以通过 setYear 来产生变化。 JSR 310 中包括的日期类型主要有：  计算机时间：Instant，对应 java.util.Date，它代表了一个确定的时间点，即相对于标准Java纪元（1970年1月1日）的偏移量；但与java.util.Date类不同的是其精确到了纳秒级别。 人类时间：对应于人类自身的观念，比如LocalDate和LocalTime。他们代表了一般的时区概念，要么是日期（不包含时间），要么是时间（不包含日期），类似于java.sql的表示方式。此外，还有一个MonthDay，它可以存储某人的生日（不包含年份）。每个类都在内部存储正确的数据而不是像java.util.Date那样利用午夜12点来区分日期，利用1970-01-01来表示时间。这些类型的实例是 immutable 的，而且只能通过工厂方法创建。  时区...</div></div></div></a><a class="pagination-related" href="/2018/10/13/%E5%8D%A1%E8%A1%A8%E5%92%8C-RSet/" title="卡表和 RSet"><img class="cover" src="/img/wall-paper-91.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2018-10-13</div><div class="info-item-2">卡表和 RSet</div></div><div class="info-2"><div class="info-item-1">卡表和 RSet 问题定义：为什么需要跨区域引用记录 JVM 垃圾收集器的核心工作之一是确定 live set——哪些对象仍然存活、不可回收。确定 live set 的标准做法是从 GC Roots（栈帧中的局部变量、静态字段等）出发，沿引用链遍历所有可达对象。 问题在于：当堆被划分为多个区域（代、Region）并且只回收其中一部分时，如何高效地找到从&quot;不回收区域&quot;指向&quot;回收区域&quot;的引用？ 以 Young GC 为例：只回收新生代，但老年代中可能持有指向新生代对象的引用。如果不处理这些跨代引用，就会错误地回收仍被老年代引用的新生代对象。最朴素的做法是扫描整个老年代来找出这些引用——但老年代通常远大于新生代，这样做的代价过高，违背了分代收集&quot;只回收一部分堆&quot;的初衷。 卡表（Card Table）和 RSet（Remembered Set）正是为解决这个问题而设计的辅助数据结构。二者的关系并非互相替代，而是层次不同、协作互补：卡表是底层的脏标记机制，RSet 是建立在卡表之上的更高层索引结构。 核心概念速览 在深入细节之前，...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">Spark SQL的发展历程</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">Spark SQL 解析流程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8sessioncatalog%E4%BF%9D%E5%AD%98%E5%85%83%E6%95%B0%E6%8D%AE"><span class="toc-number">2.1.</span> <span class="toc-text">使用SessionCatalog保存元数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E6%9E%90-sql-%E4%BD%BF%E7%94%A8-antlr-%E7%94%9F%E6%88%90%E6%9C%AA%E8%A7%A3%E6%9E%90%E7%9A%84%E9%80%BB%E8%BE%91%E8%AE%A1%E5%88%92"><span class="toc-number">2.2.</span> <span class="toc-text">解析 SQL: 使用 ANTLR 生成未解析的逻辑计划</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%88%86%E6%9E%90%E5%99%A8-analyzer-%E8%A7%A3%E6%9E%90%E9%80%BB%E8%BE%91%E8%AE%A1%E5%88%92"><span class="toc-number">2.3.</span> <span class="toc-text">使用分析器 Analyzer 解析逻辑计划</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E4%BC%98%E5%8C%96%E5%99%A8-optimizer-%E4%BC%98%E5%8C%96%E9%80%BB%E8%BE%91%E8%AE%A1%E5%88%92"><span class="toc-number">2.4.</span> <span class="toc-text">使用优化器 Optimizer 优化逻辑计划</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8sparkplanner%E7%94%9F%E6%88%90%E7%89%A9%E7%90%86%E8%AE%A1%E5%88%92"><span class="toc-number">2.5.</span> <span class="toc-text">使用SparkPlanner生成物理计划</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-queryexecution-%E6%89%A7%E8%A1%8C%E7%89%A9%E7%90%86%E8%AE%A1%E5%88%92"><span class="toc-number">2.6.</span> <span class="toc-text">使用 QueryExecution 执行物理计划</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">三 SparkSQL中物理计划的执行</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#spark-%E7%9A%84%E6%A0%B9%E5%9F%BA-rdd"><span class="toc-number">3.1.</span> <span class="toc-text">Spark 的根基: RDD</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8E%E7%89%A9%E7%90%86%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92%E5%88%B0-rdd"><span class="toc-number">3.2.</span> <span class="toc-text">从物理执行计划到 RDD</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">SparkSQL中任务的执行模式</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">SparkSQL 中任务的具体执行</span></a></li></ol></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2017 - 2026 By magicliang</span><span class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.1.1</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.2</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Toggle Between Traditional and Simplified Chinese">簡</button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.2"></script><script src="/js/main.js?v=5.5.2"></script><script src="/js/tw_cn.js?v=5.5.2"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const config = mermaidSrc.dataset.config ? JSON.parse(mermaidSrc.dataset.config) : {}
      if (!config.theme) {
        config.theme = theme
      }
      const mermaidThemeConfig = `%%{init: ${JSON.stringify(config)}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.12.1/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="/"></script></div></body></html>